{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "407ceca1",
   "metadata": {},
   "source": [
    "# ECS764P Applied Statistics - Coursework 2\n",
    "\n",
    "- **Professors:** Dr. Frederik Dahlqvist\n",
    "- **Due Date:** Thursday, 15. January 2026 (12:00 GMT)\n",
    "- **Student:** Philipp Schmidt\n",
    "- **Python Version:** 3.11.14\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e67dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the required dependencies\n",
    "import os\n",
    "\n",
    "if os.path.exists('requirements.txt'):\n",
    "    %pip install -q -r requirements.txt\n",
    "else:\n",
    "    print(\"requirements.txt not found. Installing packages manually...\")\n",
    "    %pip install -q matplotlib numpy pandas requests scipy tqdm yfinance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618318e4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 1: Explain Dataset Choice\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6be8e59",
   "metadata": {},
   "source": [
    "We want to perform a test to check for the correlation between a stock's investor attention and its volatility at the London Stock Exchange.\n",
    "\n",
    "We assume there is a decent volatility observable as usually news drives both attention (visible in Wikipedia visits) and stock price moves. \n",
    "Unusual attention spikes might indicate news events, earnings announcements, or sentiment shifts that drive trading activity.\n",
    "\n",
    "We use the absolute value of log returns as a standard measure of realized volatility and the daily percentage change in Wikipedia pageviews to capture sudden spikes or drops in public interest that may correlate with market movements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a7cb62",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 2: Fetch and Preprocess Data from External API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6267910",
   "metadata": {},
   "source": [
    "For this we are taking the biggest stocks traded at the London Stock Exchange by market capitalisation (based on Dec. 2025).\n",
    "The task only asks for one stock but, out of personal interest, we’re doing more (if you only want one then set `use_multiple_stocks = False`).\n",
    "\n",
    "We fetch the data using the `yfinance` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ebfc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag\n",
    "use_multiple_stocks = True\n",
    "\n",
    "if not(use_multiple_stocks):\n",
    "    stocks = {\n",
    "        \"RR.L\": \"Rolls-Royce_Holdings\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c496c1ad",
   "metadata": {},
   "source": [
    "To make sure that the dataset is available during the grading process and the risk that Wikimedia API requests are rate-limited/refused, as every date has to get fetched and this might trigger the API's bots policy we chose the way of uploading the data beforehand as a `.csv` file to GitHub and now fetching it from there.\n",
    "If you want to directly fetch the Wikimedia API set `use_wikimedia_api = True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de970cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag\n",
    "use_wikimedia_api = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49734da5",
   "metadata": {},
   "source": [
    "For each stock entry we want to retrieve the page views of their Wikipedia website and calculate a percentage change of the days before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf32a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the stocks we want to analyze with their Wikipedia article names\n",
    "# These are the largest stocks on the London Stock Exchange by market cap (Dec 2025)\n",
    "if use_multiple_stocks:\n",
    "    stocks = {\n",
    "        \"AZN.L\": \"AstraZeneca\",\n",
    "        \"HSBA.L\": \"HSBC\",\n",
    "        \"SHEL.L\": \"Shell_plc\",\n",
    "        \"ULVR.L\": \"Unilever\",\n",
    "        \"RR.L\": \"Rolls-Royce_Holdings\",\n",
    "        \"BATS.L\": \"British_American_Tobacco\",\n",
    "        \"RIO.L\": \"Rio_Tinto_(corporation)\",\n",
    "        \"GSK.L\": \"GSK_plc\",\n",
    "        \"BP.L\": \"BP\",\n",
    "        \"BARC.L\": \"Barclays\",\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec75fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import urllib.parse\n",
    "import yfinance as yf\n",
    "from tqdm import tqdm\n",
    "\n",
    "def fetch_stock_data_from_external_apis(stocks: dict) -> Tuple[pd.DataFrame, Path]:\n",
    "    \"\"\"\n",
    "    Fetch stock price data and Wikipedia pageview statistics for given stock tickers.\n",
    "    \n",
    "    This function retrieves historical stock closing prices using yfinance, calculates\n",
    "    logarithmic returns, and fetches Wikipedia pageview counts for the corresponding\n",
    "    company articles. It also computes the daily percentage change in pageviews.\n",
    "    \n",
    "    Args\n",
    "    ----\n",
    "    stocks : dict\n",
    "        Dictionary mapping stock tickers (str) to Wikipedia article names (str).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    stock_information_df: pd.DataFrame\n",
    "        Multi-index DataFrame with columns:\n",
    "            - ('Log_Returns', ticker): Daily logarithmic returns.\n",
    "            - ('Views', ticker): Wikipedia pageviews count.\n",
    "            - ('Views_Change', ticker): Daily percentage change in pageviews.\n",
    "    \n",
    "    stock_information_csv: Path\n",
    "        Path pointing to the CSV file where the DataFrame is saved.\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    RuntimeError\n",
    "        If yfinance returns no data for the provided tickers.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract ticker symbols from the input dictionary\n",
    "    stock_tickers = list(stocks.keys())\n",
    "    \n",
    "    # Download daily stock price data from yfinance\n",
    "    dl = yf.download(stock_tickers, start='2023-11-29', end='2025-12-02', interval=\"1d\", auto_adjust=True)\n",
    "    if dl is None or (hasattr(dl, \"empty\") and dl.empty):\n",
    "        raise RuntimeError(f\"yfinance returned no data for tickers: {stock_tickers}\")\n",
    "    \n",
    "    try:\n",
    "        # Extract the 'Close' price column and remove any missing values\n",
    "        stock_information_df = dl[\"Close\"].dropna()\n",
    "    except Exception:\n",
    "        # Handle MultiIndex columns that yfinance sometimes returns\n",
    "        stock_information_df = dl.xs(\"Close\", axis=1, level=0, drop_level=True).dropna()\n",
    "\n",
    "    # Dictionary to hold all computed columns for the final DataFrame\n",
    "    df_dict = {}\n",
    "\n",
    "    # Compute logarithmic returns for each stock\n",
    "    # Log returns = ln(P_t / P_{t-1}) where P_t is the closing price at time t\n",
    "    for ticker in stock_tickers:\n",
    "        df_dict[('Log_Returns', ticker)] = np.log(stock_information_df[ticker] / stock_information_df[ticker].shift(1))\n",
    "\n",
    "    # Initialise Wikipedia pageviews column with NaN values\n",
    "    for ticker in stock_tickers:\n",
    "        df_dict[('Views', ticker)] = np.nan\n",
    "    \n",
    "    # Initialise Wikipedia pageviews change column with NaN values\n",
    "    for ticker in stock_tickers:\n",
    "        df_dict[('Views_Change', ticker)] = np.nan\n",
    "    \n",
    "    # Create a new DataFrame with multi-index columns containing all metrics\n",
    "    stock_information_df = pd.DataFrame(df_dict, index=stock_information_df.index)\n",
    "\n",
    "    # Delete first row as it doesn't contain a valid log return (requires previous day)\n",
    "    stock_information_df = stock_information_df.iloc[1:]\n",
    "    \n",
    "    # Set up HTTP headers for Wikimedia API requests\n",
    "    # User-Agent is required by Wikimedia's bot policy\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"User-Agent\": \"qmul-student-agent/1.0 (mailto:student@qmul.ac.uk)\",\n",
    "    }\n",
    "    \n",
    "    # Create a session to reuse connections and headers across requests\n",
    "    session = requests.Session()\n",
    "    session.headers.update(headers)\n",
    "    \n",
    "    # Get all dates for which we need to fetch Wikipedia data\n",
    "    search_dates = stock_information_df.index\n",
    "    \n",
    "    # Calculate total API requests needed for progress tracking\n",
    "    total_requests = len(stocks) * len(search_dates)\n",
    "    pbar = tqdm(total=total_requests, desc=\"Fetching Wikipedia pageviews\")\n",
    "\n",
    "    # Iterate through each stock and fetch Wikipedia pageview data\n",
    "    for ticker, article in stocks.items():\n",
    "        # URL-encode the Wikipedia article name to handle special characters\n",
    "        article_enc = urllib.parse.quote(str(article), safe=\"\")\n",
    "        \n",
    "        # Fetch pageview data for each date\n",
    "        for date in search_dates:\n",
    "            # Format date as YYYYMMDD for the Wikimedia API\n",
    "            d = pd.Timestamp(date).strftime(\"%Y%m%d\")\n",
    "            \n",
    "            # Construct the Wikimedia Pageviews API URL\n",
    "            # Fetches daily pageviews for a specific article on a specific date\n",
    "            url = (\n",
    "                \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/\"\n",
    "                f\"en.wikipedia.org/all-access/all-agents/{article_enc}/daily/{d}/{d}\"\n",
    "            )\n",
    "            \n",
    "            # Make the API request\n",
    "            response = session.get(url)\n",
    "            \n",
    "            # Parse the response if successful\n",
    "            if response.ok:\n",
    "                data = response.json()\n",
    "                items = data.get(\"items\", [])\n",
    "                # Extract views count from the first item, or None if no data\n",
    "                views = items[0][\"views\"] if items else None\n",
    "            else:\n",
    "                views = None\n",
    "                print(f\"\\nError fetching {ticker} on {date.strftime('%Y-%m-%d')}: {response.status_code}\")\n",
    "\n",
    "            # Store the pageview count in the DataFrame\n",
    "            stock_information_df.loc[date, ('Views', ticker)] = views\n",
    "            \n",
    "            # Update progress bar with current stock and date\n",
    "            pbar.set_postfix_str(f\"{ticker} - {date.strftime('%Y-%m-%d')}\")\n",
    "            pbar.update(1)\n",
    "    \n",
    "    # Close the progress bar\n",
    "    pbar.close()\n",
    "    \n",
    "    # Calculate the daily percentage change in Wikipedia pageviews\n",
    "    # Formula: (Views_t - Views_{t-1}) / Views_{t-1}\n",
    "    for ticker in stock_tickers:\n",
    "        views_col = ('Views', ticker)\n",
    "        views_change_col = ('Views_Change', ticker)\n",
    "        \n",
    "        stock_information_df[views_change_col] = (\n",
    "            (stock_information_df[views_col] - stock_information_df[views_col].shift(1))\n",
    "            / stock_information_df[views_col].shift(1)\n",
    "        )\n",
    "    \n",
    "    # Delete first row as it doesn't contain a valid views change (requires previous day)\n",
    "    stock_information_df = stock_information_df.iloc[1:]\n",
    "    \n",
    "    # Write CSV to a Path and return that Path (to_csv returns None when writing to a file)\n",
    "    csv_path = Path('ecs764p-applied-statistics-coursework-2-dataset.csv')\n",
    "    stock_information_df.to_csv(csv_path, index=True)\n",
    "    \n",
    "    return stock_information_df, csv_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8673cd8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def fetch_data_from_github() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Fetch pre-computed stock and Wikipedia data from GitHub Gist.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame containing stock returns and Wikipedia pageview metrics.\n",
    "    \"\"\"\n",
    "    path = 'https://gist.githubusercontent.com/PhilippXXY/4511ed662bc78c847d9d65adf610d591/raw/d4dec838c9caba906b1ebd3c1ad1f25774f5f53f/ecs764p-applied-statistics-coursework-2-dataset.csv'\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        index_col=0,\n",
    "        parse_dates=True,\n",
    "        date_format='%Y-%m-%d',\n",
    "        header=[0, 1],\n",
    "        skiprows=[2],\n",
    "    )\n",
    "    \n",
    "    df.index.name = 'Date'\n",
    "    \n",
    "    # Clean up column names by stripping whitespace\n",
    "    df.columns = pd.MultiIndex.from_tuples([\n",
    "    (level0.strip(), level1.strip()) \n",
    "    for level0, level1 in df.columns\n",
    "    ])\n",
    "    \n",
    "    # Convert all columns to numeric types\n",
    "    for col in df.columns:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7536cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_wikimedia_api:\n",
    "    df, _ = fetch_stock_data_from_external_apis(stocks)    \n",
    "else:\n",
    "    df = fetch_data_from_github()\n",
    "\n",
    "# Drop absolute views column\n",
    "df = df.drop(columns='Views')\n",
    "\n",
    "# Add absolute returns (volatility proxy) to the dataframe\n",
    "for ticker in stocks.keys():\n",
    "    df[('Abs_Log_Returns', ticker)] = df[('Log_Returns', ticker)].abs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d44f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns: Log_Returns, Abs_Log_Returns, Views_Change\n",
    "new_column_order = []\n",
    "\n",
    "for ticker in stocks.keys():\n",
    "    new_column_order.append(('Log_Returns', ticker))\n",
    "    new_column_order.append(('Abs_Log_Returns', ticker))\n",
    "    new_column_order.append(('Views_Change', ticker))\n",
    "\n",
    "# Reindex the DataFrame with the new column order\n",
    "df = df[new_column_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f769e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display data\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3767bd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get basic descriptive statistics\n",
    "descriptive_stats_df = df.describe(percentiles=[.01, .05, .25, .5, .75, .95, 0.99])\n",
    "descriptive_stats_df.loc['skew'] = df.skew()\n",
    "descriptive_stats_df.loc['kurtosis'] = df.kurtosis()\n",
    "\n",
    "print(\"*** Display descriptive statistics for all stocks ***\")\n",
    "display(descriptive_stats_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecd43d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all columns where the second level of the MultiIndex is 'AZN.L'\n",
    "print(\"*** Display descriptive statistics for AstraZeneca only ***\")\n",
    "display(descriptive_stats_df.xs('RR.L', level=1, axis=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef15e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Scatter plots: Views Change vs Volatility (Absolute Returns)\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ticker in enumerate(stocks.keys()):\n",
    "    x = df[('Views_Change', ticker)]\n",
    "    y = df[('Abs_Log_Returns', ticker)]\n",
    "    \n",
    "    axes[i].scatter(x, y, alpha=0.5, s=20)\n",
    "    axes[i].set_xlabel('Wikipedia Views Change', fontsize=9)\n",
    "    axes[i].set_ylabel('Absolute Returns (Volatility)', fontsize=9)\n",
    "    axes[i].set_title(ticker, fontsize=11, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].axvline(x=0, color='red', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "    \n",
    "    corr = x.corr(y)\n",
    "    axes[i].text(0.05, 0.95, f'ρ = {corr:.3f}', \n",
    "                 transform=axes[i].transAxes, \n",
    "                 verticalalignment='top',\n",
    "                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.suptitle('Stock Volatility vs Wikipedia Views Change', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8197f180",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import gaussian_kde\n",
    "\n",
    "# Density contour plots: Views Change vs Volatility (Absolute Returns)\n",
    "fig, axes = plt.subplots(2, 5, figsize=(18, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ticker in enumerate(stocks.keys()):\n",
    "    x = df[('Views_Change', ticker)].dropna()\n",
    "    y = df[('Abs_Log_Returns', ticker)].dropna()\n",
    "    \n",
    "    # Align the data (remove rows where either x or y is NaN)\n",
    "    valid_idx = x.index.intersection(y.index)\n",
    "    x = x.loc[valid_idx]\n",
    "    y = y.loc[valid_idx]\n",
    "    \n",
    "    # Calculate kernel density estimate for contours\n",
    "    # Stack the data\n",
    "    xy = np.vstack([x, y])\n",
    "    \n",
    "    # Calculate kernel density\n",
    "    kde = gaussian_kde(xy)\n",
    "    \n",
    "    # Create grid for contour plot\n",
    "    x_min, x_max = x.min(), x.max()\n",
    "    y_min, y_max = y.min(), y.max()\n",
    "    \n",
    "    # Add some padding\n",
    "    x_range = x_max - x_min\n",
    "    y_range = y_max - y_min\n",
    "    x_min -= 0.2 * x_range\n",
    "    x_max += 0.2 * x_range\n",
    "    y_min -= 0.2 * y_range\n",
    "    y_max += 0.2 * y_range\n",
    "    \n",
    "    xx, yy = np.meshgrid(\n",
    "        np.linspace(x_min, x_max, 200),\n",
    "        np.linspace(y_min, y_max, 200)\n",
    "    )\n",
    "    \n",
    "    # Evaluate KDE on grid\n",
    "    positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "    zz = kde(positions).reshape(xx.shape)\n",
    "    \n",
    "    contourf = axes[i].contourf(xx, yy, zz, levels=15, cmap='viridis', alpha=0.9)\n",
    "    \n",
    "    axes[i].set_xlabel('Wikipedia Views Change', fontsize=9)\n",
    "    axes[i].set_ylabel('Absolute Returns (Volatility)', fontsize=9)\n",
    "    axes[i].set_title(ticker, fontsize=11, fontweight='bold')\n",
    "    axes[i].axvline(x=0, color='red', linestyle='--', linewidth=1, alpha=0.7)\n",
    "    axes[i].axhline(y=y.median(), color='white', linestyle='--', linewidth=0.8, alpha=0.5)\n",
    "    \n",
    "    # Add correlation coefficient\n",
    "    corr = x.corr(y)\n",
    "    axes[i].text(0.05, 0.95, f'ρ = {corr:.3f}', \n",
    "                 transform=axes[i].transAxes, \n",
    "                 verticalalignment='top',\n",
    "                 bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                 fontsize=9)\n",
    "\n",
    "plt.suptitle('Density Distribution: Stock Volatility vs Wikipedia Views Change', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd3615b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Calculate high and low attention volatility for each stock\n",
    "high_vol = []\n",
    "low_vol = []\n",
    "ticker_names = []\n",
    "\n",
    "for ticker in stocks.keys():\n",
    "    high_attention = df[df[('Views_Change', ticker)] > df[('Views_Change', ticker)].quantile(0.75)]\n",
    "    low_attention = df[df[('Views_Change', ticker)] < df[('Views_Change', ticker)].quantile(0.25)]\n",
    "    \n",
    "    high_vol.append(high_attention[('Abs_Log_Returns', ticker)].mean())\n",
    "    low_vol.append(low_attention[('Abs_Log_Returns', ticker)].mean())\n",
    "    ticker_names.append(ticker)\n",
    "\n",
    "# Calculate percentage differences\n",
    "pct_diffs = [((high_vol[i] - low_vol[i]) / low_vol[i]) * 100 for i in range(len(ticker_names))]\n",
    "\n",
    "# Create bar plot\n",
    "x = np.arange(len(ticker_names))\n",
    "width = 0.35\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "bars1 = ax.bar(x - width/2, high_vol, width, label='High Attention (Top 25%)', alpha=0.8, color='#e74c3c')\n",
    "bars2 = ax.bar(x + width/2, low_vol, width, label='Low Attention (Bottom 25%)', alpha=0.8, color='#3498db')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "# Add percentage difference labels above each pair of bars\n",
    "for i, (pct, ticker) in enumerate(zip(pct_diffs, ticker_names)):\n",
    "    max_height = max(high_vol[i], low_vol[i])\n",
    "    color = '#27ae60' if pct > 0 else '#c0392b'\n",
    "    ax.text(i, max_height * 1.05, f'{pct:+.1f}%', \n",
    "            ha='center', va='bottom', fontsize=9, fontweight='bold', color=color)\n",
    "\n",
    "ax.set_xlabel('Stock Ticker', fontsize=11)\n",
    "ax.set_ylabel('Average Absolute Returns (Volatility)', fontsize=11)\n",
    "ax.set_title('Stock Volatility: High vs Low Wikipedia Attention Days', fontsize=13, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(ticker_names, rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add some padding at the top for percentage labels\n",
    "ax.set_ylim(top=max(max(high_vol), max(low_vol)) * 1.15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420e4d90",
   "metadata": {},
   "source": [
    "The graphics show that we can expect some observable correlation for some stocks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829ff898",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 3: Hypothesis Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc21ee8d",
   "metadata": {},
   "source": [
    "### Significance level\n",
    "\n",
    "$$\n",
    "\\alpha = 0.05.\n",
    "$$\n",
    "\n",
    "### Hypotheses\n",
    "\n",
    "We perform a two-sided hypothesis test to assess whether there is a linear association between the two\n",
    "one-dimensional datasets:\n",
    "\n",
    "$$\n",
    "H_{0}:\\ \\rho = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_{1}:\\ \\rho \\neq 0 .\n",
    "$$\n",
    "\n",
    "### Test statistic (Lecture 10: t-test for slope in simple linear regression)\n",
    "\n",
    "Equivalently, in the simple linear regression model $Y = \\beta_{0} + \\beta_{1}X + \\varepsilon$, testing for\n",
    "no linear association corresponds to testing whether the slope is zero:\n",
    "\n",
    "$$\n",
    "H_{0}:\\ \\beta_{1} = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_{1}:\\ \\beta_{1} \\neq 0 .\n",
    "$$\n",
    "\n",
    "We use the $t$-test for the regression slope. The test statistic is\n",
    "\n",
    "$$\n",
    "t \\;=\\; \\frac{\\widehat{\\beta}_{1}-0}{\\operatorname{se}(\\widehat{\\beta}_{1})},\n",
    "$$\n",
    "\n",
    "and under $H_{0}$ it follows a Student $t$-distribution with $n-2$ degrees of freedom:\n",
    "\n",
    "$$\n",
    "t \\sim t_{n-2}.\n",
    "$$\n",
    "\n",
    "### Critical region\n",
    "\n",
    "For a two-sided test at level $\\alpha$, the critical value is $t_{1-\\alpha/2,\\;n-2}$, hence the rejection\n",
    "region is\n",
    "\n",
    "$$\n",
    "|t| > t_{1-\\alpha/2,\\;n-2},\n",
    "$$\n",
    "\n",
    "equivalently,\n",
    "\n",
    "$$\n",
    "t \\notin \\left[-t_{1-\\alpha/2,\\;n-2},\\; t_{1-\\alpha/2,\\;n-2}\\right].\n",
    "$$\n",
    "\n",
    "### p-value\n",
    "\n",
    "Given an observed value $t_{\\mathrm{obs}}$, the two-sided $p$-value is\n",
    "\n",
    "$$\n",
    "p \\;=\\; 2\\left(1 - F_{t_{n-2}}\\!\\left(|t_{\\mathrm{obs}}|\\right)\\right),\n",
    "$$\n",
    "\n",
    "where $F_{t_{n-2}}$ denotes the CDF of the Student $t$-distribution with $n-2$ degrees of freedom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7017ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def perform_hypothesis_test(x, y, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Perform t-test for regression slope to test correlation.\n",
    "    \n",
    "    H0: β1 = 0 (no linear association)\n",
    "    H1: β1 ≠ 0 (linear association exists)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : array-like\n",
    "        Independent variable (Views_Change)\n",
    "    y : array-like\n",
    "        Dependent variable (Abs_Log_Returns)\n",
    "    alpha : float\n",
    "        Significance level (default 0.05)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary containing test results\n",
    "    \"\"\"\n",
    "    # Remove NaN values\n",
    "    valid_idx = ~(np.isnan(x) | np.isnan(y))\n",
    "    x_clean = x[valid_idx]\n",
    "    y_clean = y[valid_idx]\n",
    "    \n",
    "    n = len(x_clean)\n",
    "    \n",
    "    # Calculate regression slope and intercept\n",
    "    x_mean = np.mean(x_clean)\n",
    "    y_mean = np.mean(y_clean)\n",
    "    \n",
    "    # β1 = Σ[(xi - x̄)(yi - ȳ)] / Σ[(xi - x̄)²]\n",
    "    numerator = np.sum((x_clean - x_mean) * (y_clean - y_mean))\n",
    "    denominator = np.sum((x_clean - x_mean) ** 2)\n",
    "    beta_1 = numerator / denominator\n",
    "    \n",
    "    # β0 = ȳ - β1 * x̄\n",
    "    beta_0 = y_mean - beta_1 * x_mean\n",
    "    \n",
    "    # Calculate fitted values and residuals\n",
    "    y_fitted = beta_0 + beta_1 * x_clean\n",
    "    residuals = y_clean - y_fitted\n",
    "    \n",
    "    # Calculate residual standard error\n",
    "    SSE = np.sum(residuals ** 2)\n",
    "    sigma_squared = SSE / (n - 2)\n",
    "    \n",
    "    # Calculate standard error of β1\n",
    "    se_beta_1 = np.sqrt(sigma_squared / denominator)\n",
    "    \n",
    "    # Calculate t-statistic\n",
    "    t_stat = beta_1 / se_beta_1\n",
    "    \n",
    "    # Degrees of freedom\n",
    "    dof = n - 2\n",
    "    \n",
    "    # Calculate p-value (two-sided)\n",
    "    p_value = 2 * (1 - stats.t.cdf(np.abs(t_stat), dof))\n",
    "    \n",
    "    # Critical value\n",
    "    t_critical = stats.t.ppf(1 - alpha/2, dof)\n",
    "    \n",
    "    # Decision\n",
    "    reject_null = np.abs(t_stat) > t_critical\n",
    "    \n",
    "    # Calculate correlation coefficient\n",
    "    correlation = np.corrcoef(x_clean, y_clean)[0, 1]\n",
    "    \n",
    "    return {\n",
    "        'n': n,\n",
    "        'beta_0': beta_0,\n",
    "        'beta_1': beta_1,\n",
    "        'se_beta_1': se_beta_1,\n",
    "        't_statistic': t_stat,\n",
    "        'dof': dof,\n",
    "        'p_value': p_value,\n",
    "        't_critical': t_critical,\n",
    "        'reject_null': reject_null,\n",
    "        'correlation': correlation,\n",
    "        'alpha': alpha\n",
    "    }\n",
    "\n",
    "\n",
    "# Perform hypothesis test for each stock\n",
    "results = {}\n",
    "\n",
    "alpha = 0.05 \n",
    "for ticker in stocks.keys():\n",
    "    x = df[('Views_Change', ticker)].values\n",
    "    y = df[('Abs_Log_Returns', ticker)].values\n",
    "    \n",
    "    result = perform_hypothesis_test(x, y, alpha)\n",
    "    results[ticker] = result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3f8d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary table of results\n",
    "import pandas as pd\n",
    "\n",
    "summary_data = []\n",
    "for ticker in stocks.keys():\n",
    "    res = results[ticker]\n",
    "    summary_data.append({\n",
    "        'Stock': ticker,\n",
    "        'Company': stocks[ticker].replace('_', ' '),\n",
    "        'n': res['n'],\n",
    "        'Correlation ρ': res['correlation'],\n",
    "        'Slope β₁': res['beta_1'],\n",
    "        't-stat': res['t_statistic'],\n",
    "        'p-value': res['p_value'],\n",
    "        'Reject H₀': 'yes' if res['reject_null'] else 'no'\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "tickers_list = list(stocks.keys())\n",
    "significant_count = sum(1 for t in tickers_list if results[t]['reject_null'])\n",
    "\n",
    "print(\"\\nSUMMARY TABLE\")\n",
    "print(\"=\" * 100)\n",
    "display(summary_df)\n",
    "print(f\"For {significant_count} out of {len(tickers_list)} stocks H₀ has to be rejected at a significance level of α = {alpha}.\")\n",
    "print(\"=\" * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "fig, axes = plt.subplots(4, 3, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, ticker in enumerate(stocks.keys()):\n",
    "    res = results[ticker]\n",
    "    \n",
    "    # Get test statistics\n",
    "    t_obs = res['t_statistic']\n",
    "    t_crit = res['t_critical']\n",
    "    dof = res['dof']\n",
    "    p_val = res['p_value']\n",
    "    \n",
    "    # Create x-axis values for the t-distribution\n",
    "    x_max = max(abs(t_obs), t_crit) * 1.5\n",
    "    x = np.linspace(-x_max, x_max, 1000)\n",
    "    \n",
    "    # Calculate the t-distribution PDF\n",
    "    y = stats.t.pdf(x, dof)\n",
    "    \n",
    "    # Plot the PDF\n",
    "    axes[i].plot(x, y, 'k-', linewidth=1.5, label='t-distribution PDF')\n",
    "    \n",
    "    # Shade critical regions (rejection regions)\n",
    "    x_left_crit = x[x <= -t_crit]\n",
    "    y_left_crit = stats.t.pdf(x_left_crit, dof)\n",
    "    axes[i].fill_between(x_left_crit, y_left_crit, alpha=0.3, color='red', label='Critical region')\n",
    "    \n",
    "    x_right_crit = x[x >= t_crit]\n",
    "    y_right_crit = stats.t.pdf(x_right_crit, dof)\n",
    "    axes[i].fill_between(x_right_crit, y_right_crit, alpha=0.3, color='red')\n",
    "    \n",
    "    # Shade p-value regions\n",
    "    if t_obs > 0:\n",
    "        x_p_right = x[x >= abs(t_obs)]\n",
    "        y_p_right = stats.t.pdf(x_p_right, dof)\n",
    "        axes[i].fill_between(x_p_right, y_p_right, alpha=0.5, color='orange', label='p-value region')\n",
    "        \n",
    "        x_p_left = x[x <= -abs(t_obs)]\n",
    "        y_p_left = stats.t.pdf(x_p_left, dof)\n",
    "        axes[i].fill_between(x_p_left, y_p_left, alpha=0.5, color='orange')\n",
    "    else:\n",
    "        x_p_left = x[x <= -abs(t_obs)]\n",
    "        y_p_left = stats.t.pdf(x_p_left, dof)\n",
    "        axes[i].fill_between(x_p_left, y_p_left, alpha=0.5, color='orange', label='p-value region')\n",
    "        \n",
    "        x_p_right = x[x >= abs(t_obs)]\n",
    "        y_p_right = stats.t.pdf(x_p_right, dof)\n",
    "        axes[i].fill_between(x_p_right, y_p_right, alpha=0.5, color='orange')\n",
    "    \n",
    "    # Mark the observed t-statistic\n",
    "    axes[i].axvline(x=t_obs, color='blue', linestyle='-', linewidth=2, label=f't = {t_obs:.3f}')\n",
    "    \n",
    "    # Mark critical values\n",
    "    axes[i].axvline(x=-t_crit, color='red', linestyle='--', linewidth=1, alpha=0.7)\n",
    "    axes[i].axvline(x=t_crit, color='red', linestyle='--', linewidth=1, alpha=0.7)\n",
    "    \n",
    "    # Labels and title\n",
    "    axes[i].set_xlabel('t-statistic', fontsize=9)\n",
    "    axes[i].set_ylabel('Probability Density', fontsize=9)\n",
    "    axes[i].set_title(f'{ticker}\\np = {p_val:.4f}', fontsize=10, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend only to first subplot\n",
    "    if i == 0:\n",
    "        axes[i].legend(fontsize=7, loc='upper right')\n",
    "    \n",
    "    # Add text box with decision\n",
    "    decision_text = 'Reject H₀' if res['reject_null'] else 'Fail to reject H₀'\n",
    "    decision_color = '#27ae60' if res['reject_null'] else '#95a5a6'\n",
    "    axes[i].text(0.05, 0.95, decision_text,\n",
    "                 transform=axes[i].transAxes,\n",
    "                 verticalalignment='top',\n",
    "                 bbox=dict(boxstyle='round', facecolor=decision_color, alpha=0.7),\n",
    "                 fontsize=8, color='white', fontweight='bold')\n",
    "\n",
    "# Hide unused subplots\n",
    "for j in range(len(stocks), len(axes)):\n",
    "    axes[j].set_visible(False)\n",
    "\n",
    "plt.suptitle(f't-Test for Regression Slope: Test Statistic on t-distribution PDF (df = n-2)', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3803b08b",
   "metadata": {},
   "source": [
    "We can see that for some stocks, the null hypothesis must be rejected.\n",
    "This means there’s no evidence that they’re not linearly dependent.\n",
    "\n",
    "For the others we fail to reject $H_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb42e075",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 4: Linear Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270fb7e4",
   "metadata": {},
   "source": [
    "Having found in the previous step that we can’t reject the null hypothesis for all stocks, and observing no obvious non-linear correlation in the scatterplots, we’ll proceed with option a) and perform a linear regression on these few stocks where we rejected $H_0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eff352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Get stocks with significant linear relationship\n",
    "significant_stocks = [ticker for ticker in stocks.keys() if results[ticker]['reject_null']]\n",
    "\n",
    "print(f\"Performing linear regression for {len(significant_stocks)} stocks with significant correlation:\")\n",
    "print(f\"{', '.join(significant_stocks)}\\n\")\n",
    "\n",
    "# Store regression results\n",
    "regression_results = {}\n",
    "regression_summary = []\n",
    "\n",
    "for ticker in significant_stocks:\n",
    "    # Get data\n",
    "    x = df[('Views_Change', ticker)].to_numpy(dtype=float)\n",
    "    y = df[('Abs_Log_Returns', ticker)].to_numpy(dtype=float)\n",
    "    \n",
    "    # Remove NaN values\n",
    "    valid_idx = np.isfinite(x) & np.isfinite(y)\n",
    "    x_clean = x[valid_idx]\n",
    "    y_clean = y[valid_idx]\n",
    "    \n",
    "    # Perform linear regression\n",
    "    slope, intercept, r_value, p_value, std_err = stats.linregress(x_clean, y_clean)\n",
    "    \n",
    "    # Calculate fitted values and residuals\n",
    "    y_fitted = intercept + slope * x_clean\n",
    "    residuals = y_clean - y_fitted\n",
    "    \n",
    "    n = len(x_clean)\n",
    "    RSS = np.sum(residuals**2)\n",
    "    residuals_std = np.sqrt(RSS / (n - 2))\n",
    "    \n",
    "    # Store results\n",
    "    regression_results[ticker] = {\n",
    "        'x': x_clean,\n",
    "        'y': y_clean,\n",
    "        'slope': slope,\n",
    "        'intercept': intercept,\n",
    "        'r_value': r_value,\n",
    "        'p_value': p_value,\n",
    "        'std_err': std_err,\n",
    "        'y_fitted': y_fitted,\n",
    "        'residuals': residuals,\n",
    "        'residuals_std': residuals_std\n",
    "    }\n",
    "    \n",
    "    # Add to summary table\n",
    "    regression_summary.append({\n",
    "        'Stock': ticker,\n",
    "        'n': n,\n",
    "        'Intercept (β₀)': intercept,\n",
    "        'Slope (β₁)': slope,\n",
    "        'R²': r_value**2,\n",
    "        'Residuals Std (s)': residuals_std,\n",
    "        'Regression Equation': f'y = {intercept:.4f} + {slope:.4f}x'\n",
    "    })\n",
    "\n",
    "# Display as table\n",
    "regression_summary_df = pd.DataFrame(regression_summary)\n",
    "display(regression_summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60c065c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot regression results: scatter plots with regression lines\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, ticker in enumerate(significant_stocks):\n",
    "    res = regression_results[ticker]\n",
    "    \n",
    "    # Scatter plot\n",
    "    axes[i].scatter(res['x'], res['y'], alpha=0.5, s=30, label='Data points')\n",
    "    \n",
    "    # Regression line\n",
    "    x_line = np.array([res['x'].min(), res['x'].max()])\n",
    "    y_line = res['intercept'] + res['slope'] * x_line\n",
    "    axes[i].plot(x_line, y_line, 'r-', linewidth=2, label='Regression line')\n",
    "    \n",
    "    # Labels and title\n",
    "    axes[i].set_xlabel('Wikipedia Views Change', fontsize=11)\n",
    "    axes[i].set_ylabel('Absolute Returns (Volatility)', fontsize=11)\n",
    "    axes[i].set_title(f'{ticker}\\ny = {res[\"intercept\"]:.4f} + {res[\"slope\"]:.4f}x\\nR² = {res[\"r_value\"]**2:.4f}',\n",
    "                     fontsize=11, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].legend(loc='upper left', fontsize=9)\n",
    "\n",
    "plt.suptitle('Linear Regression: Volatility vs Wikipedia Views Change', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d9c6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms of residuals\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, ticker in enumerate(significant_stocks):\n",
    "    res = regression_results[ticker]\n",
    "    residuals = res['residuals']\n",
    "    \n",
    "    # Histogram\n",
    "    n, bins, patches = axes[i].hist(residuals, bins=30, density=True, alpha=0.7, \n",
    "                                     color='skyblue', edgecolor='black', label='Residuals')\n",
    "    \n",
    "    # Overlay normal distribution\n",
    "    mu = 0\n",
    "    sigma = res['residuals_std']\n",
    "    x_norm = np.linspace(residuals.min(), residuals.max(), 100)\n",
    "    y_norm = stats.norm.pdf(x_norm, mu, sigma)\n",
    "    axes[i].plot(x_norm, y_norm, 'r-', linewidth=2, label=f'N(0, {sigma:.4f})')\n",
    "    \n",
    "    # Labels and title\n",
    "    axes[i].set_xlabel('Residuals', fontsize=11)\n",
    "    axes[i].set_ylabel('Density', fontsize=11)\n",
    "    axes[i].set_title(f'{ticker}\\nMean = {np.mean(residuals):.6f}, Std = {sigma:.6f}',\n",
    "                     fontsize=11, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].legend(loc='upper right', fontsize=9)\n",
    "    axes[i].axvline(x=0, color='green', linestyle='--', linewidth=1, alpha=0.5)\n",
    "\n",
    "plt.suptitle('Histogram of Residuals from Linear Regression', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40684ab9",
   "metadata": {},
   "source": [
    "Now we perform the Kolmogorov-Smirnov Test for Normality of Residuals with\n",
    "\n",
    "$$\n",
    "H_{0}: \\; \\text{Residuals come from N($0$, $s^2$) distribution}\n",
    "$$\n",
    "\n",
    "$$\n",
    "H_{1}: \\; \\text{Residuals do not come from N($0$, $s^2$) distribution}\n",
    "$$\n",
    "\n",
    "at a significance level of $\\alpha = 0.05$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35522249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kolmogorov-Smirnov test for normality of residuals\n",
    "from scipy.stats import kstest\n",
    "\n",
    "ks_results = {}\n",
    "ks_summary = []\n",
    "\n",
    "for ticker in significant_stocks:\n",
    "    res = regression_results[ticker]\n",
    "    residuals = res['residuals']\n",
    "    sigma = res['residuals_std']\n",
    "    \n",
    "    # Standardise residuals\n",
    "    standardized_residuals = residuals / sigma\n",
    "    \n",
    "    # Perform KS test against standard normal distribution N(0,1)\n",
    "    # Then we test if residuals/s ~ N(0,1), equivalent to residuals ~ N(0, s²)\n",
    "    ks_statistic, p_value = kstest(standardized_residuals, 'norm', args=(0, 1))\n",
    "    \n",
    "    # Critical value for KS test at α = 0.05\n",
    "    n = len(residuals)\n",
    "    # Kolmogorov-Smirnov critical value approximation\n",
    "    critical_value = 1.36 / np.sqrt(n)\n",
    "    \n",
    "    reject_null = ks_statistic > critical_value\n",
    "    \n",
    "    ks_results[ticker] = {\n",
    "        'ks_statistic': ks_statistic,\n",
    "        'p_value': p_value,\n",
    "        'critical_value': critical_value,\n",
    "        'reject_null': reject_null,\n",
    "        'n': n\n",
    "    }\n",
    "    \n",
    "    # Add to summary table\n",
    "    decision = 'Reject H₀' if reject_null else 'Fail to reject H₀'\n",
    "    conclusion = 'not Normal' if reject_null else 'Consistent with Normal'\n",
    "    \n",
    "    ks_summary.append({\n",
    "        'Stock': ticker,\n",
    "        'n': n,\n",
    "        'KS Statistic (D)': ks_statistic,\n",
    "        'Critical Value': critical_value,\n",
    "        'p-value': p_value,\n",
    "        'Decision': decision,\n",
    "        'Conclusion': conclusion\n",
    "    })\n",
    "\n",
    "# Display as table\n",
    "ks_summary_df = pd.DataFrame(ks_summary)\n",
    "display(ks_summary_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7ee7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot KS test statistic on Kolmogorov distribution\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import kstwobign\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, ticker in enumerate(significant_stocks):\n",
    "    ks_res = ks_results[ticker]\n",
    "    \n",
    "    D_obs = ks_res['ks_statistic']\n",
    "    D_crit = ks_res['critical_value']\n",
    "    n = ks_res['n']\n",
    "    \n",
    "    # --- Use asymptotic Kolmogorov distribution of X = sqrt(n) * D ---\n",
    "    x_obs = np.sqrt(n) * D_obs\n",
    "    x_crit = np.sqrt(n) * D_crit\n",
    "\n",
    "    # --- p-value consistent with the Kolmogorov PDF you plot ---\n",
    "    # (right-tail area beyond observed statistic)\n",
    "    p_val = kstwobign.sf(x_obs)\n",
    "\n",
    "    # Create x grid\n",
    "    x_max = max(x_obs, x_crit) * 2\n",
    "    x_max = max(x_max, 3.0)  # ensure a reasonable plot range\n",
    "    x = np.linspace(0, x_max, 2000)\n",
    "\n",
    "    # PDF for kstwobign is available directly\n",
    "    pdf_vals = kstwobign.pdf(x)\n",
    "\n",
    "    # Plot the PDF\n",
    "    axes[i].plot(x, pdf_vals, 'k-', linewidth=1.5, label='Kolmogorov PDF')\n",
    "\n",
    "    # Shade critical region: X >= x_crit\n",
    "    mask_crit = x >= x_crit\n",
    "    axes[i].fill_between(x[mask_crit], pdf_vals[mask_crit], alpha=0.3, color='red', label='Critical region')\n",
    "\n",
    "    # Shade p-value region: X >= x_obs\n",
    "    mask_p = x >= x_obs\n",
    "    axes[i].fill_between(x[mask_p], pdf_vals[mask_p], alpha=0.5, color='orange', label='p-value region')\n",
    "\n",
    "    # Mark observed and critical values\n",
    "    axes[i].axvline(x=x_obs, color='blue', linestyle='-', linewidth=2, label=f'√n·D = {x_obs:.3f}')\n",
    "    axes[i].axvline(x=x_crit, color='red', linestyle='--', linewidth=1.5, alpha=0.7, label='Critical value')\n",
    "\n",
    "    # Decision\n",
    "    reject_null = x_obs > x_crit\n",
    "    decision_text = 'Reject H₀' if reject_null else 'Fail to reject H₀'\n",
    "    decision_color = '#e74c3c' if reject_null else '#27ae60'\n",
    "\n",
    "    axes[i].set_xlabel('√n · D', fontsize=11)\n",
    "    axes[i].set_ylabel('Probability Density', fontsize=11)\n",
    "    axes[i].set_title(f'{ticker}\\nD = {D_obs:.4f}, p = {p_val:.4f}', fontsize=11, fontweight='bold')\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "    axes[i].legend(loc='upper right', fontsize=8)\n",
    "\n",
    "    axes[i].text(\n",
    "        0.05, 0.95, decision_text,\n",
    "        transform=axes[i].transAxes,\n",
    "        verticalalignment='top',\n",
    "        bbox=dict(boxstyle='round', facecolor=decision_color, alpha=0.7),\n",
    "        fontsize=9, color='white', fontweight='bold'\n",
    "    )\n",
    "\n",
    "plt.suptitle('Kolmogorov-Smirnov Test: Normality of Residuals (asymptotic Kolmogorov law)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1091e40",
   "metadata": {},
   "source": [
    "### Conclusions from Task 4a\n",
    "\n",
    "For the three stocks that showed significant linear correlation in Task 3 (HSBC, Rolls-Royce, and British American Tobacco), we performed linear regression analysis and tested the normality of residuals.\n",
    "\n",
    "**Linear Regression Results:**\n",
    "\n",
    "- **HSBC (HSBA.L)**: $y = 0.0096 + 0.0115x$, $R^2 = 0.029$\n",
    "- **Rolls-Royce (RR.L)**: $y = 0.0145 + 0.0162x$, $R^2 = 0.030$\n",
    "- **British American Tobacco (BATS.L)**: $y = 0.0092 + 0.0219x$, $R^2 = 0.086$\n",
    "\n",
    "All three stocks show positive slopes, confirming that increased Wikipedia attention is associated with higher stock volatility. However, the $R^2$ values are quite low ($2.9\\%-8.6\\%$), indicating that Wikipedia views change explains only a small portion of volatility variance or the other way around.\n",
    "\n",
    "**Kolmogorov-Smirnov Test for Normality:**\n",
    "\n",
    "All three stocks **reject the null hypothesis** at $\\alpha = 0.05$, with very small $p$-values ($p < 0.0001$):\n",
    "\n",
    "- **HSBC**: $D = 0.173$, $p < 0.0001$\n",
    "- **Rolls-Royce**: $D = 0.137$, $p < 0.0001$\n",
    "- **British American Tobacco**: $D = 0.128$, $p < 0.0001$\n",
    "\n",
    "**Interpretation:**\n",
    "\n",
    "The residuals from the linear regression do **not** follow a normal distribution with mean zero and standard deviation s. This suggests that:\n",
    "\n",
    "1. The linear model may not fully capture the relationship between Wikipedia attention and stock volatility\n",
    "2. The distribution of residuals shows significant departures from normality, visible in the histograms as positive skewness and heavy tails\n",
    "3. The normality assumption required for standard linear regression inference is violated\n",
    "\n",
    "This finding suggests that while there is a statistically significant linear association, the relationship may be more complex or contain outliers that violate the classical linear regression assumptions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ffc51c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Task 5:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d73991",
   "metadata": {},
   "source": [
    "Independence means the joint distribution factorises as a product of its marginals, as there is no \"information flow\" between the two.\n",
    "But marginals alone are not enough to decide independence because very different joint distributions can share the same marginals.\n",
    "So, to test independence we would compare the empirical joint distribution to the product of the empirical marginals.\n",
    "\n",
    "For this we perform a Pearson's $\\chi^2$ goodness-of-fit test:\n",
    "\n",
    "1. Split $X$ and $Y$ into bins.\n",
    "2. Build the contingency table $O_{ij}$ that represents the observed counts in bin $(i,j)$.\n",
    "3. Under $H_0$ (independence) the expected counts are $E_{ij} = n \\hat{p}_{i} \\hat{q}_{j}$ where $\\hat{p}_{i}$ and $\\hat{q}_{j}$ are the empirical marginals.\n",
    "4. Use the Pearson statistic with $T = \\sum_{ij}\\frac{\\left(O_{ij} - E_{ij}\\right)^2}{E_{ij}}$\n",
    "5. Compute a p-value from a $\\chi^2$ distribution and reject if $p < \\alpha = 0.05$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c373bec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency, chi2\n",
    "\n",
    "# Significance level\n",
    "alpha = 0.05\n",
    "# Number of bins\n",
    "kx = ky = 20\n",
    "\n",
    "independence_summary = []\n",
    "for i, ticker in enumerate(stocks.keys()):\n",
    "\n",
    "    # Get series\n",
    "    x = df[('Views_Change', ticker)]\n",
    "    y = df[('Abs_Log_Returns', ticker)]\n",
    "\n",
    "    # Drop NaNs + align\n",
    "    x = x.dropna()\n",
    "    y = y.dropna()\n",
    "    valid_idx = x.index.intersection(y.index)\n",
    "    x = x.loc[valid_idx]\n",
    "    y = y.loc[valid_idx]\n",
    "\n",
    "    # Create bins\n",
    "    x_bin = pd.qcut(x, q=kx, duplicates=\"drop\")\n",
    "    y_bin = pd.qcut(y, q=ky, duplicates=\"drop\")\n",
    "    # Create crosstab\n",
    "    obs = pd.crosstab(x_bin, y_bin)\n",
    "\n",
    "    # Chi-Square test\n",
    "    chi2_stat, p_value, dof, expected = chi2_contingency(obs.values, correction=False)\n",
    "\n",
    "    # Critical value (right-tailed)\n",
    "    critical_value = chi2.ppf(1 - alpha, dof)\n",
    "\n",
    "    reject_null = p_value < alpha\n",
    "\n",
    "    # Add to summary table\n",
    "    decision = 'Reject H₀' if reject_null else 'Fail to reject H₀'\n",
    "    conclusion = 'Dependent' if reject_null else 'Insufficient evidence of dependence'\n",
    "\n",
    "    independence_summary.append({\n",
    "        'Stock': ticker,\n",
    "        'n': len(x),\n",
    "        'dof': dof,\n",
    "        'Chi2': chi2_stat,\n",
    "        'Critical Value': critical_value,\n",
    "        'p-value': p_value,\n",
    "        'Decision': decision,\n",
    "        'Conclusion': conclusion\n",
    "    })\n",
    "\n",
    "# Display as table\n",
    "independence_summary_df = pd.DataFrame(independence_summary)\n",
    "display(independence_summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998c1003",
   "metadata": {},
   "source": [
    "We fail to reject independence at $\\alpha=0.05$.\n",
    "The data are consistent with independence under this $\\chi^2$ binning approach, but this does not prove independence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
